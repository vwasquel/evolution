{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://github.com/d9w/evolution/raw/master/imgs/logo.png\" width=\"20%\" align=\"right\" style=\"margin:0px 20px\">\n",
    "\n",
    "\n",
    "# Evolutionary Algorithms\n",
    "\n",
    "## Symbolic Regression\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-sa/4.0/80x15.png\" /></a>&nbsp;| Dennis G. Wilson | <a href=\"https://d9w.github.io/evolution/\">https://d9w.github.io/evolution/</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This notebook is partially adapted from Michael Littman's \"Introduction to Machine Learning\" [class](https://github.com/mlittmancs/great_courses_ml)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OYhWPv_Sxmhn",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this class, we will be exploring symbolic regression: the evolution of a symbolic function that inputs and outputs data. We will use the `gplearn` [package](https://gplearn.readthedocs.io/en/stable/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!pip install gplearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pBhW9Zp8P13h",
    "outputId": "1f117f63-2a31-4a3b-922f-876961e0860f",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from gplearn.genetic import SymbolicRegressor\n",
    "from gplearn.genetic import SymbolicClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.utils.random import check_random_state\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In the first example, we will train a symbolic regression using genetic programming.  We will try to fit our model to a set of points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0KCDbr61OjM",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "First, we create the function for our data and make our training and test data with the function `fitme`. Given an input `x`, our target function will be a simple cubic function: $\\frac{x^3}{10} + x^2$.\n",
    "\n",
    "We’ll make a training set `X_train` for this function consisting of 50 `uniform` random `xs` between -10 and 10. We run these points through our target `fitme` function to get the corresponding targets, `y_train`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XxKjFjGsM4pk",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def fitme(x):\n",
    "    return 0.1*x*x*x + x*x\n",
    "\n",
    "# Training samples\n",
    "X_train = np.random.uniform(-10, 10, (50,1))\n",
    "y_train = [fitme(X) for X in X_train]\n",
    "\n",
    "# Testing samples\n",
    "X_test = np.random.uniform(-10, 10, (50,1))\n",
    "y_test = [fitme(X) for X in X_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AiBDHHOp6ad4",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will plot the data below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "Q8Pi7NXvUQTc",
    "outputId": "5658544c-34b0-46cf-87b3-f8182f270824",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(X_train, y_train)\n",
    "plt.title('Target distance')\n",
    "plt.xlabel('angle')\n",
    "plt.ylabel('distance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's fit a model to this function using genetic programming. Our genetic-programming-based estimator, `est_gp` is a “symbolic regressor”. That is to say, we’re solving a regression problem by finding a symbolic expression --- a little piece of a program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-zroV7F6cs0",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The program it finds will use only two operators: `add` for add and `mul` for multiply. A program can use as many of these operators as necessary. However, there’s a tradeoff in the search between accurately matching the training data and being parsimonious --- using a small expression.  The `parsimony_coefficient` tells the genetic programming search how much weight to put on parsimony (complexity) compared to accuracy. Parsimony coefficient can be any non-negative value, where smaller numbers for parsimony tend to result in much bigger programs created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 130
    },
    "id": "hH3deSG8M4vQ",
    "outputId": "b3df2f3b-cbca-411f-8fcf-81c3ddd8653e",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "est_gp = SymbolicRegressor(population_size=10000,parsimony_coefficient=0.1,\n",
    "                           function_set=('add', 'mul'))\n",
    "est_gp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "um9DW3om6prk",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can now plot the performance of the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "o2DBso_iXjKB",
    "outputId": "c2629b13-e566-408f-a50f-e386067917b2",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_lots = np.reshape(np.sort(np.random.uniform(-10, 10, 250)),(-1,1))\n",
    "\n",
    "y_gp = est_gp.predict(X_lots)\n",
    "\n",
    "plt.scatter(X_test, y_test)\n",
    "plt.plot(X_lots, y_gp)\n",
    "plt.title('Target distance')\n",
    "plt.xlabel('angle')\n",
    "plt.ylabel('distance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmK768JP6_uT",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Finally we print the function learned by the genetic program below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tSrdTemIM4yO",
    "outputId": "dd94d5f2-faad-4c72-ed72-c4fc8161d166",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(est_gp._program)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dot_data = est_gp._program.export_graphviz()\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We expect to find an expression that is similar to the original:  $\\frac{x^3}{10} + x^2$. Do you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Exercise 1</h3>\n",
    "\n",
    "Change the function to $\\sin(\\frac{x^2}{10}) + \\cos(\\frac{x}{2})$. Can you reproduce this function using the same method? What parameters of the algorithm should change to make this easier?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's now look at an example where we don't know what the underlying function is: data analysis. We will use a commonly studied problem of breast cancer detection. In medical applications such as cancer detection, explainability of the machine learning model is critical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "rng = check_random_state(0)\n",
    "cancer = load_breast_cancer()\n",
    "perm = rng.permutation(cancer.target.size)\n",
    "cancer.data = cancer.data[perm]\n",
    "cancer.target = cancer.target[perm]\n",
    "cancer.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The goal of this dataset is to determine if a cell is malignant or benign. The features are the following measurements of a cell nucleus, taken over three dimensions:\n",
    "\n",
    "+ radius (mean of distances from center to points on the perimeter)\n",
    "+ texture (standard deviation of gray-scale values)\n",
    "+ perimeter\n",
    "+ area\n",
    "+ smoothness (local variation in radius lengths)\n",
    "+ compactness (perimeter^2 / area - 1.0)\n",
    "+ concavity (severity of concave portions of the contour)\n",
    "+ concave points (number of concave portions of the contour)\n",
    "+ symmetry\n",
    "+ fractal dimension (\"coastline approximation\" - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "cancer.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "est = SymbolicClassifier(parsimony_coefficient=.01,\n",
    "                         feature_names=cancer.feature_names,\n",
    "                         random_state=1)\n",
    "est.fit(cancer.data[:400], cancer.target[:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The fitting process can be long, but let's see how the evolved model performs on unseen data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_true = cancer.target[400:]\n",
    "y_score = est.predict_proba(cancer.data[400:])[:,1]\n",
    "print('\\n'.join([str((y_true[i], y_score[i])) for i in range(5)]))\n",
    "roc_auc_score(y_true, y_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is a good accuracy! We can also see that the probabilities usually match the classification, although there are some incorrectly classified cases. Let's see if we can use the graph to understand how classification is being performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dot_data = est._program.export_graphviz()\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A different type of explainable machine learning model is the Decision Tree. We won't get into the details of how they're made, but feel free to look at the [scikit-learn documentation](https://scikit-learn.org/stable/modules/tree.html) for more information. Decision Trees use information theory to split features at relevant points in order to construct rules based on inequalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "decision_tree = DecisionTreeClassifier(random_state=0, max_depth=2)\n",
    "decision_tree = decision_tree.fit(cancer.data[:400], cancer.target[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_true = cancer.target[400:]\n",
    "y_score = decision_tree.predict_proba(cancer.data[400:])[:,1]\n",
    "print('\\n'.join([str((y_true[i], y_score[i])) for i in range(5)]))\n",
    "roc_auc_score(y_true, y_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import graphviz \n",
    "dot_data = export_graphviz(decision_tree, out_file=None) \n",
    "graph = graphviz.Source(dot_data) \n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Exercise 2</h3>\n",
    "\n",
    "As we can see, the GP graph uses a number of different functions, like multiplication and addition, in its tree, where the Decision Tree uses inequalities. Which leads to higher explainability, in your opinion? Discuss in groups.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Bonus Exercise</h3>\n",
    "\n",
    "Try tuning the parameters of the GP training. If you allow for larger trees, does this result in better models in terms of test accuracy?\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "name": "L07.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
