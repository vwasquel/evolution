{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../imgs/logo.png\" width=\"20%\" align=\"right\" style=\"margin:0px 20px\">\n",
    "\n",
    "\n",
    "# Evolutionary Computation\n",
    "\n",
    "## Project: Evolution of Agents\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-sa/4.0/80x15.png\" /></a>&nbsp;| Dennis G. Wilson | <a href=\"https://d9w.github.io/evolution/\">https://d9w.github.io/evolution/</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Outline\n",
    "\n",
    "1. [Reinforcement Learning](#rl)\n",
    "2. [Evolving Agents](#evolving)\n",
    "3. [Atari games](#atari)\n",
    "4. [CGP](#cgp)\n",
    "5. [Neuroevolution](#neuroevo)\n",
    "6. [Comparison](#comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In this project, you have been asked to evolve an agent to play a video game. In this notebook, we'll see an example of that as well as a brief explanation on agent evolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <a id=\"rl\"></a> Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The field of Reinforcement Learning provides a good framework for understanding the actions of an agent in an environment which rewards specific actions. There are many resources about RL online, including Emmanuel Rachelson's [class](https://github.com/erachelson/RLclass) which is taught in the SDD Master's program at Supaero. We'll look at a brief overview as it relates to evolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In RL, an agent acts based on a policy $\\pi$, which uses state information $S_t$ to take an action $A_t$. Based on this action, the environment rewards the agent $R_{t+1}$ and the agent moves into a new state $S_{t+1}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"../imgs/erl.png\" width=\"35%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The standard objective function for an evolutionary algorithm when applied to this problem is the sum total of the reward over all timesteps in an episode, from $t=0$ to the end of the episode $t=T$.\n",
    "\n",
    "$$\\sum_{t=0}^{t=T} R_{t}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In other words, evolution is aiming to maximize the total reward over all states $S$ observed and actions $A$ taken  during this episode. The set of states observed and actions taken depends on the current individual $\\pi$ (or policy), where an action $a$ for a given state $s$ is determined by $a=\\pi(s)$. A different way to write the objective function is:\n",
    "\n",
    "$$\\sum_{t=0}^{t=T} r(s_t, \\pi(s_t))$$\n",
    "\n",
    "where $r$ is the reward function of the problem. The optimization of the total reward over the entire episode is very different from Reinforcement Learning algorithms, which maximize the reward over individual state, action $(S,A)$ pairs and many of which aim to approximate $r$. In most evolutionary computation, all that matters is the total reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Reminder</h3>\n",
    "    \n",
    "In an evolutionary algorithm, the total reward is not the only fitness objective possible. Quality diversity algorithms like MAP-Elites use behavior metrics which measure a different objective than reward over an episode. Using total reward is a good way to compare evolutionary methods to Reinforcement Learning algorithms, but it is not the only option for evolutionary fitness functions.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <a id=\"evolving\"></a>Evolving Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "When evolving agents, there are many options for representing the agent. The agent needs only to be able to taken an action $a$ at every step $s$, in other words the agent is $\\pi$ and we'll use the agent to compute $a=\\pi(s)$. Any representation of a function is possible: a layered artificial neural network where weights are optimized by evolution; a complex ANN where structure and weights are evolved; a genetic program; a gene regulatory network; any evolvable function representation works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../imgs/agent_representations.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The evaluation of an individual is therefore a multi-step process. Given an individual's genes $G$, a phenotype $P$ is computed. The genes can take many forms, as can the phenotypes. One example is a binary gene regulatory network, where binary genes encode protein tags which inform the weights of the final network, which is the phenotype. Another is the integer genotype used in NEAT, which encodes neurons and their connections in the phenotype which is a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"../imgs/gene_pheno_behavior.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " For evaluation, $P$ must be capable of representing a function $\\pi$ which computes $a=\\pi(s)$ or sometimes $a_t=\\pi(s_t, r(s_{t-1}, a_{t-1}))$, where the reward from the previous step is also provided. The evaluation of the agent which informs its chances of passing on genetic information is then based on its behavior in a simulated environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In this notebook, we'll compare Cartesian Genetic Programming, where our agent is a function graph, and neuroevolution, using a $(1, \\lambda)$ evolutionary strategy to evolve the weights of a 2 layer neural network. We'll use Atari games as the environment, specifically Ms. Pacman."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <a id=\"atari\"></a> Atari games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For a game environment, we'll use the [Arcade Learning Environment](https://github.com/mgbellemare/Arcade-Learning-Environment), which allows us to emulate and manipulate Atari games. There is a [Julia wrapper](https://github.com/JuliaReinforcementLearning/ArcadeLearningEnvironment.jl) which I've only tested on Linux, although the [atari-py](https://github.com/openai/atari-py) package mentions Windows support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "using Pkg\n",
    "pkg\"add ArcadeLearningEnvironment\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This package automatically installs a number of ROMs from the `atari-py` repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "using ArcadeLearningEnvironment\n",
    "ale = ALE_new()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "getROMList()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We'll look at  Ms. Pacman as an example. Instead of using the entire action set, we'll only use the actions available in this game. Since the \"fire\" button isn't used in Ms. Pacman, the only actions are the 8 directions from the joystick and no input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "loadROM(ale, \"ms_pacman\")\n",
    "actions = getMinimalActionSet(ale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's see what the environment looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "using Images, ImageView\n",
    "function get_screen(ale)\n",
    "    screen = reshape(getScreenRGB(ale), (3, getScreenWidth(ale), getScreenHeight(ale))) ./ 256\n",
    "    colorview(RGB, permutedims(screen, [1, 3, 2]))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "get_screen(ale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's watch the environment as our agent takes random actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "using Reel\n",
    "Reel.set_output_type(\"gif\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "loadROM(ale, \"ms_pacman\")\n",
    "frames = Frames(MIME(\"image/png\"), fps=60)\n",
    "total_reward = 0\n",
    "while ~game_over(ale)\n",
    "    total_reward += act(ale, rand(actions))\n",
    "    push!(frames, get_screen(ale))\n",
    "end\n",
    "reset_game(ale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For the state of our environment, we have two options: the pixels, as we saw in the [class on CGP](https://d9w.github.io/evolution/4_gp/2_cgp.html), and the RAM, the memory of the game. We'll use the RAM in this example since it is much smaller and evolution will be faster since we don't need to get the screen every step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "loadROM(ale, \"ms_pacman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "getRAM(ale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <a id=\"cgp\"></a>CGP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's evolve our first agent. We'll use `Cambrian.jl` and `CartesianGeneticProgramming.jl` to evolve a function graph using CGP which takes in the RAM values and outputs a value per action, which we'll use the maximum of to determine the action at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pkg\"add https://github.com/d9w/Cambrian.jl\"\n",
    "pkg\"add https://github.com/d9w/CartesianGeneticProgramming.jl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "using Cambrian\n",
    "using CartesianGeneticProgramming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "An example configuration for CGP on this problem is provided in `atari.yaml`. This specifies which functions CGP can choose from, how many rows and columns there are, and what the mutation rate is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cfg = get_config(\"atari.yaml\")\n",
    "cfg[\"n_in\"] = length(getRAM(ale))\n",
    "cfg[\"n_out\"] = length(actions)\n",
    "cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We'll define our objective function allowing us to also use this function to visualize an agent as it plays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cgp_fits = []\n",
    "function play_atari(ind::CGPInd; render=false, seed=0)\n",
    "    ale = ALE_new()\n",
    "    loadROM(ale, \"ms_pacman\")\n",
    "    if render\n",
    "        frames = Frames(MIME(\"image/png\"), fps=6)\n",
    "    end\n",
    "    f = 0\n",
    "    total_reward = 0\n",
    "    while ~game_over(ale) && f < 18000\n",
    "        inputs = getRAM(ale) ./ typemax(UInt8)\n",
    "        output = process(ind, inputs)\n",
    "        total_reward += act(ale, actions[argmax(output)])\n",
    "        if render && mod(f, 10) == 0\n",
    "            push!(frames, get_screen(ale))\n",
    "        end\n",
    "        f += 1\n",
    "    end\n",
    "    ALE_del(ale)\n",
    "    if render\n",
    "        return total_reward, frames\n",
    "    end\n",
    "    push!(cgp_fits, total_reward)\n",
    "    [total_reward]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's see how a random individual does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ind = CGPInd(cfg);\n",
    "@timev total_reward, frames = play_atari(ind; render=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The rendering evaluation took a while if we consider that during evolution, we'll run this function hundreds or thousands of times. Let's see how long it takes without rendering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "@timev total_reward = play_atari(ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "That's better. Let's run the evolution for 20 generations, evaluating 5 individuals each time ($\\lambda$=5) for a total of 100 evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cgp_fits = []\n",
    "evo = CartesianGeneticProgramming.evolution(cfg, play_atari);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for i in 1:20\n",
    "    Cambrian.step!(evo)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's observe the best individual from evolution and display its actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "best = sort(evo.population)[end];\n",
    "total_reward, frames = play_atari(best, render=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <a id=\"neuroevo\"></a> Neuroevolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For our second approach, we'll evolve the weights of a neural network using the $(1, \\lambda)$ evolutionary strategy we in the [evolutionary strategies tutorial](https://github.com/d9w/evolution/blob/master/5_strategies/1_ES.ipynb). For the neural network, we'll use [Flux](fluxml.ai) to define the neural network - we'll use a 2 layer network with (32, 16) neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "using Flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n_in = length(getRAM(ale))\n",
    "n_out = length(actions)\n",
    "model = Chain(\n",
    "    Dense(n_in, 32),\n",
    "    Dense(32, 16),\n",
    "    Dense(16, n_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can test this neural network with some game inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model(getRAM(ale) ./ typemax(UInt8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We need to redefine our Atari function to use a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "function play_atari(model::Chain; render=false, seed=0)\n",
    "    loadROM(ale, \"ms_pacman\")\n",
    "    if render\n",
    "        frames = Frames(MIME(\"image/png\"), fps=6)\n",
    "    end\n",
    "    f = 0\n",
    "    total_reward = 0\n",
    "    while ~game_over(ale) && f < 18000\n",
    "        inputs = getRAM(ale) ./ typemax(UInt8)\n",
    "        output = model(inputs)\n",
    "        total_reward += act(ale, actions[argmax(output)])\n",
    "        if render && mod(f, 10) == 0\n",
    "            push!(frames, get_screen(ale))\n",
    "        end\n",
    "        f += 1\n",
    "    end\n",
    "    reset_game(ale)\n",
    "    if render\n",
    "        return total_reward, frames\n",
    "    end\n",
    "    total_reward\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's see how a random neural network plays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "total_reward, frames = play_atari(model, render=true)\n",
    "total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We're not quite ready to evolve the weights, however. To do that, we need to make a function that transforms genes into a neural network. Let's see how many parameters our network has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "param_lengths = [length(p) for p in params(model)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ndims = sum(param_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So we'll optimize 4809 genes and use them as the weights and biases of our neural network (the phenotype) to evaluate them. We need to defined a function that takes in the genes and returns a neural network with those genes and weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "function set_weights(x::Array{Float64})\n",
    "    model = Chain(\n",
    "        Dense(n_in, 32),\n",
    "        Dense(32, 16),\n",
    "        Dense(16, n_out))\n",
    "    start = 1\n",
    "    for p in params(model)\n",
    "        p .= reshape(x[start:(start+length(p)-1)], size(p))\n",
    "        start += length(p)\n",
    "    end\n",
    "    model\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's see if that worked. If we set all weights to 0, the output should always be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model = set_weights(zeros(ndims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model(rand(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now that we have our genotype to phenotype mapping function, we can create the objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "function objective(x::Array{Float64})\n",
    "    model = set_weights(x)\n",
    "    play_atari(model)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Finally, we'll import the ES step function from the evolutionary strategies tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "using Statistics\n",
    "using LinearAlgebra\n",
    "function step(x::Array{Float64}, objective::Function; npop=20, sigma=0.5, alpha=0.1)\n",
    "    N = randn(npop, length(x))\n",
    "    P = repeat(x, 1, npop)' .+ sigma .* N\n",
    "    R = zeros(npop)\n",
    "    for i in eachindex(R)\n",
    "        R[i] = objective(P[i, :])\n",
    "    end\n",
    "    A = (R .- mean(R)) ./ std(R)\n",
    "        \n",
    "    x .+ alpha/(npop * sigma) .* [dot(N[:, i], A) for i in 1:size(N, 2)], R, P\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's create a random individual and see its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x = rand(ndims);\n",
    "objective(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Not great, but it'll do as a starting point. We'll run the first generation and get back $x$, the new center of our population, $R$, the array of all fitness values, and $P$, the population matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x, R, P = step(x, objective);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's see what the best individual we found was. Note that this isn't our new $x$, its whichever individual we observed with the best fitness. $x$ should have moved in the direction of that individual's genes, but is also based on fitness values from the entire population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "(maximum(R), argmax(R))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We'll run this for 4 more generations, keeping track of the best individual each generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "es_fits = []\n",
    "append!(es_fits, R)\n",
    "best_ind = P[argmax(R), :]\n",
    "best_fit = maximum(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for i in 2:5\n",
    "    x, R, P = step(x, objective)\n",
    "    append!(es_fits, R)\n",
    "    if maximum(R) > best_fit\n",
    "        best_fit = maximum(R)\n",
    "        best_ind .= P[argmax(R), :]\n",
    "    end\n",
    "    println(best_fit, length(es_fits))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Once that's done, let's see how our best individual does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model = set_weights(best_ind)\n",
    "total_reward, frames = play_atari(model, render=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <a id=\"comparison\"></a> Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now that we have run two different evolutionary methods, we can compare them. Let's look at all 100 evaluations for both method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "using Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plot(cgp_fits, label=\"CGP\")\n",
    "plot!(es_fits, label=\"ES\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Seeing each individual evaluation is useful for understanding our optimization behavior. For example, with CGP, we can note that once it finds a good performing individual, it will test many other individuals which appear to have exactly the same behavior. Furthermore, most CGP mutations greatly decrease the overall fitness, bringing it down to the random action baseline of 60 or 70."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For the neuroevolutionary strategy, we see that it doesn't build well off of previously observed individuals, with many sampled individuals at the random baseline. While the center of the genes is moving along the approximated gradient, the points sampled are chosen randomly and could have very low fitness values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "These sort of insights are useful for improving evolutionary methods. Finally, to better compare the two methods, we'll look at only the maximum fitness found over the entire evolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot(accumulate(max, cgp_fits), label=\"CGP\")\n",
    "plot!(accumulate(max, es_fits), label=\"ES\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "CGP is a clear winner here, but of course to do a fair comparison, we should run both algorithms multiple times and then compare the average best fitness found and the standard deviation, ideally using a statistical significance test to see if the results are truly different. For environments with a known maximum fitness, the percentage of average evolutions which reach the maximum is also a very useful metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    Using the software provided in the project page, experiment with one or multiple evolutionary methods for the evolution of a game-playing agent. In your video presentations, please show and discuss:\n",
    "    <ul>\n",
    "        <li>Which evolutionary algorithm was used, with a brief overview of the algorithm</li>\n",
    "        <li>What, if any, modifications were made to the algorithm</li>\n",
    "        <li>What algorithm parameters were used and if you experimented with the parameters</li>\n",
    "        <li>If you compared to a second method, these details about the second method</li>\n",
    "        <li>Describe the game you were using</li>\n",
    "        <li>The state, actions, and objective function for the game</li>\n",
    "        <li>If any behavior characteristic was used, describe it</li>\n",
    "        <li>The results of evolution showing multiple trials with statistical analysis</li>\n",
    "        <li>Your interpretation of the results</li>\n",
    "        <li>Video of your agent playing the game</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As a bonus, let's see an agent that evolved for 10000 generations (to 50000 total evaluations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "using JSON\n",
    "best = JSON.parsefile(\"best.dna\")\n",
    "ind = CGPInd(cfg, Float64.(best[\"chromosome\"]));\n",
    "total_reward, frames = play_atari(ind, render=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Good luck and have fun!\n",
    "\n",
    "<img src=\"../imgs/cat.gif\" width=\"30%\">"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.4.1",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
