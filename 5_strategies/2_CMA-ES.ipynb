{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../imgs/logo.png\" width=\"20%\" align=\"right\" style=\"margin:0px 20px\">\n",
    "\n",
    "\n",
    "# Evolutionary Computation\n",
    "\n",
    "## 5.2 CMA-ES\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-sa/4.0/80x15.png\" /></a>&nbsp;| Dennis G. Wilson | <a href=\"https://d9w.github.io/evolution/\">https://d9w.github.io/evolution/</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMA-ES\n",
    "\n",
    "In this section, we'll discuss the Covariance Matrix Adaptation Evolutionary Strategy, or CMA-ES [1, 2]. This is one of the most well-known evolutionary algorithms in general and is a state-of-the-art algorithm for continuous optimization. The strength of this method is that it adapts the distribution it uses to generate the next population based on the current distribution of individuals. In the previous section, we were limited to a Normal distribution with a fixed $\\sigma$. The adaptive distribution of CMA-ES means it will cross search spaces faster and narrow in more exactly on optimal points.\n",
    "\n",
    "[1] Hansen, Nikolaus, and Andreas Ostermeier. \"Adapting arbitrary normal mutation distributions in evolution strategies: The covariance matrix adaptation.\" Proceedings of IEEE international conference on evolutionary computation. IEEE, 1996.\n",
    "\n",
    "[2] Hansen, Nikolaus, and Andreas Ostermeier. \"Completely derandomized self-adaptation in evolution strategies.\" Evolutionary computation 9.2 (2001): 159-195."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go through a simplified version of the algorithm from [3]. There are improvements to CMA-ES beyond this basic framework, but that's beyond the scope of today. [4] provides a good review of different CMA-ES modifications.\n",
    "\n",
    "[3] Hans-Georg Beyer (2007) Evolution strategies. Scholarpedia, 2(8):1965.\n",
    "\n",
    "[4] Hansen, Nikolaus. \"The CMA evolution strategy: a comparing review.\" Towards a new evolutionary computation. Springer, Berlin, Heidelberg, 2006. 75-102."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by creating a random individual $\\mathbf{y}$ which will be our first expert. We also create a diagonal covariance matrix $\\mathbf{C}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mbox{(L1):} \\quad \n",
    "  \\forall l=1, \\ldots, \\lambda : \\;\\;\n",
    "  \\begin{cases}\n",
    "   & \\mathbf{w}_l \n",
    "          \\leftarrow \\sigma \\sqrt{ \\mathbf{C} } \\,\n",
    "          \\mathbf{N}_l(\\mathbf{0}, \\mathbf{1}),\\\\[2mm]\n",
    "   & \\mathbf{y}_l \\leftarrow \\mathbf{y}  + \\mathbf{w}_l, \\\\[2mm]\n",
    "   & F_l \\leftarrow F(\\mathbf{y}_l),\n",
    "  \\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first step, (L1), $\\lambda$ offspring $\\mathbf{y}_l$ are created by transforming standard normally distributed random vectors using a transformation matrix $\\sqrt{\\mathbf{C}}$ which is given by Cholesky decomposition of the covariance matrix $\\mathbf{C}$ and the global step size factor $\\sigma$. We also evaluate every individual, creating $F$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mbox{(L2):} \\quad \n",
    "      \\mathbf{y} \\leftarrow \\mathbf{y} + \\langle \\mathbf{w} \\rangle$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In (L2) the best $\\mu$ mutations are recombined forming the recombinant $\\mathbf{y}$ (center of mass individual) for the next generation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mbox{(L3):} \\quad \n",
    "            \\mathbf{s} \\leftarrow \\left(1-\\frac{1}{\\tau}\\right)\\mathbf{s}\n",
    "          + \\sqrt{\\frac{\\mu}{\\tau} \\left(2-\\frac{1}{\\tau}\\right)} \\,\n",
    "            \\frac{\\langle \\mathbf{w} \\rangle}{\\sigma}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector $\\langle \\mathbf{w} \\rangle$ combines individuals from two consecutive generations so $\\langle \\mathbf{w} \\rangle/\\sigma$ represents the tendency of evolution in the search space. In (L3), this information is cumulated in the $\\mathbf{s}$ vector, which exponentially decays with the time constant $\\tau$. A good default for this is $\\tau=\\sqrt{n}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mbox{(L4):} \\quad \n",
    "      \\mathbf{C} \\leftarrow \n",
    "      \\left(1-\\frac{1}{\\tau_{\\mathrm{c}}}\\right)\\mathbf{C}\n",
    "              + \\frac{1}{\\tau_{\\mathrm{c}}} \\mathbf{s} \\mathbf{s}^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In (L4), the direction vector $\\mathbf{s}$ is used to update the covariance matrix $\\mathbf{C}$ with time constant $\\tau_{\\mathrm{c}} \\propto n^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mbox{(L5):} \\quad  \n",
    "       \\mathbf{s}_\\sigma\n",
    "       \\leftarrow \\left(1-\\frac{1}{\\tau_\\sigma}\\right) \\mathbf{s}_\\sigma\n",
    "                  + \\sqrt{\\frac{\\mu}{\\tau_\\sigma}\n",
    "                    \\left(2-\\frac{1}{\\tau_\\sigma}\\right)} \\,\n",
    "                      \\langle \\mathbf{N}(\\mathbf{0}, \\mathbf{1}) \\rangle$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mbox{(L6):} \\quad  \n",
    "      \\sigma \\leftarrow \\sigma\\exp\\left[\n",
    "                        \\frac{\\| \\mathbf{s}_{\\sigma} \\|^2 - n}\n",
    "                                   {2 n \\sqrt{n} }\n",
    "                                        \\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution standard deviation $\\sigma$ is then calculated in (L5) and (L6) using the cumulated step size adaptation (CSA) technique with time constant $\\tau_\\sigma = \\sqrt{n}$ (initially $\\mathbf{s}_\\sigma = \\mathbf{0}$). $\\langle \\mathbf{N}(\\mathbf{0}, \\mathbf{1}) \\rangle$ is the distribution we calculated in (L1). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So instead of simply using a Normal distribution to create the next generation, CMA-ES transforms a normal distribution by the covariance matrix $\\mathbf{C}$. It also moves at self-adjusting $\\sigma$. This makes its movement around the search space much more effective, as it is informed by the shape of the search space given through the fitness values $F$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see an example of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "using LinearAlgebra\n",
    "using Statistics\n",
    "using Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution = [3.5, -0.2]\n",
    "sphere(x::Array{Float64}) = sum((x .- solution).^2)\n",
    "himmelblau(x::Array{Float64}) = (x[1]^2 + x[2] - 11)^2 + (x[1] + x[2]^2 - 7)^2\n",
    "styblinski_tang(x::Array{Float64}) = sum(x.^4 .- 16 .* x.^2 .+ 5 .* x) / 2.0\n",
    "rastrigin(x::Array{Float64}) = 10.0 * length(x) .+ sum((x .- solution).^2 .- 10 .* cos.(2*pi.*(x .- solution)))\n",
    "objective = sphere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with last time, we'll optimize over 2 continuous variables. We'll set default values of the time constants based on $N$. These are recommended values for CMA-ES [5]\n",
    "\n",
    "[5] Hansen, Nikolaus, Sibylle D. Müller, and Petros Koumoutsakos. \"Reducing the time complexity of the derandomized evolution strategy with covariance matrix adaptation (CMA-ES).\" Evolutionary computation 11.1 (2003): 1-18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2\n",
    "μ = 5\n",
    "λ = 10\n",
    "τ = sqrt(N)\n",
    "τ_c = N^2\n",
    "τ_σ = sqrt(N);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make our random expert and create placeholder fitness and offspring vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = randn(N)\n",
    "offspring = Array{Array{Float64}}(undef, λ)\n",
    "F = Inf .* ones(λ);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to initialize $\\mathbf{C}, \\mathbf{N}, \\mathbf{w}, \\mathbf{s}, \\mathbf{s_\\sigma},$ and $\\mathbf{\\sigma}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = Diagonal{Float64}(I, N)\n",
    "W = zeros(N, λ)\n",
    "s = zeros(N)\n",
    "s_σ = zeros(N)\n",
    "σ = 1.0\n",
    "E = zeros(N, λ);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: We calculate the offspring distribution $\\mathbf{w}$ then center it at the center-of-mass individual $\\mathbf{y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mbox{(L1):} \\quad \n",
    "  \\forall l=1, \\ldots, \\lambda : \\;\\;\n",
    "  \\begin{cases}\n",
    "   & \\mathbf{w}_l \n",
    "          \\leftarrow \\sigma \\sqrt{ \\mathbf{C} } \\,\n",
    "          \\mathbf{N}_l(\\mathbf{0}, \\mathbf{1}),\\\\[2mm]\n",
    "   & \\mathbf{y}_l \\leftarrow \\mathbf{y}  + \\mathbf{w}_l, \\\\[2mm]\n",
    "   & F_l \\leftarrow F(\\mathbf{y}_l),\n",
    "  \\end{cases}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqrt_c = cholesky((C + C') / 2.0).U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our $\\mathbf{C}$ covariance matrix is currently 1, so this first distribution will just be a normal distribution without any transformation. We will then evaluate every individual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in 1:λ\n",
    "    E[:,i] = randn(N)\n",
    "    W[:,i] = σ * (sqrt_c * E[:,i])\n",
    "    offspring[i] = y + W[:,i]\n",
    "    F_λ[i] = objective(offspring[i])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have evaluated the individuals, we will select a subset of them to inform the next generation. We'll simply use a truncation selection, taking the top $\\mu$ individuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = sortperm(F_λ)[1:μ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now step 2. We update $\\mathbf{y}$ using the top $\\mu$ individuals in $\\mathbf{w}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mbox{(L2):} \\quad \n",
    "      \\mathbf{y} \\leftarrow \\mathbf{y} + \\langle \\mathbf{w} \\rangle$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = vec(mean(W[:,idx], dims=2))\n",
    "y += w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In step 3, we update the direction vector $\\mathbf{s}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mbox{(L3):} \\quad \n",
    "            \\mathbf{s} \\leftarrow \\left(1-\\frac{1}{\\tau}\\right)\\mathbf{s}\n",
    "          + \\sqrt{\\frac{\\mu}{\\tau} \\left(2-\\frac{1}{\\tau}\\right)} \\,\n",
    "            \\frac{\\langle \\mathbf{w} \\rangle}{\\sigma}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = (1.0 - 1.0/τ)*s + (sqrt(μ/τ * (2.0 - 1.0/τ))/σ)*w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we update our covariance matrix $\\mathbf{C}$. Note that it will no longer be a diagonal matrix: our next update will use a transformed distribution to generate the population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mbox{(L4):} \\quad \n",
    "      \\mathbf{C} \\leftarrow \n",
    "      \\left(1-\\frac{1}{\\tau_{\\mathrm{c}}}\\right)\\mathbf{C}\n",
    "              + \\frac{1}{\\tau_{\\mathrm{c}}} \\mathbf{s} \\mathbf{s}^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = (1.0 - 1.0/τ_c).*C + (s./τ_c)*s'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we update the $\\sigma$, which is the standard deviation of the distribution we generate in the first step. Note that its initial value was `1.0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mbox{(L5):} \\quad  \n",
    "       \\mathbf{s}_\\sigma\n",
    "       \\leftarrow \\left(1-\\frac{1}{\\tau_\\sigma}\\right) \\mathbf{s}_\\sigma\n",
    "                  + \\sqrt{\\frac{\\mu}{\\tau_\\sigma}\n",
    "                    \\left(2-\\frac{1}{\\tau_\\sigma}\\right)} \\,\n",
    "                      \\langle \\mathbf{N}(\\mathbf{0}, \\mathbf{1}) \\rangle$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mbox{(L6):} \\quad  \n",
    "      \\sigma \\leftarrow \\sigma\\exp\\left[\n",
    "                        \\frac{\\| \\mathbf{s}_{\\sigma} \\|^2 - n}\n",
    "                                   {2 n \\sqrt{n} }\n",
    "                                        \\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ɛ = vec(mean(E[:,idx], dims=2))\n",
    "s_σ = (1.0 - 1.0/τ_σ)*s_σ + sqrt(μ/τ_σ*(2.0 - 1.0/τ_σ))*ɛ\n",
    "σ = σ*exp(((s_σ'*s_σ)[1] - N)/(2*N*sqrt(N)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a lot! Let's put it all together in an object and see how it runs over multiple iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct CMAES\n",
    "    N::Int\n",
    "    μ::Int\n",
    "    λ::Int\n",
    "    τ::Float64\n",
    "    τ_c::Float64\n",
    "    τ_σ::Float64\n",
    "    population::Array{Array{Float64}}\n",
    "    offspring::Array{Array{Float64}}\n",
    "    F_μ::Array{Float64}\n",
    "    F_λ::Array{Float64}\n",
    "    C::Array{Float64}\n",
    "    s::Array{Float64}\n",
    "    s_σ::Array{Float64}\n",
    "    σ::Float64\n",
    "    E::Array{Float64}\n",
    "    W::Array{Float64}\n",
    "    x::Array{Float64}\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function CMAES(;N=2, μ=1, λ=10, τ=sqrt(N), τ_c=N^2, τ_σ=sqrt(N))\n",
    "    x = randn(N)\n",
    "    population = fill(x, µ)\n",
    "    offspring = Array{Array{Float64}}(undef, λ)\n",
    "    F_µ = Inf .* ones(µ)\n",
    "    F_λ = Inf .* ones(λ)\n",
    "    C = Array(Diagonal{Float64}(I, N))\n",
    "    s = zeros(N)\n",
    "    s_σ = zeros(N)\n",
    "    σ = 1.0\n",
    "    E = zeros(N, λ)\n",
    "    W = zeros(N, λ);\n",
    "    CMAES(N, μ, λ, τ, τ_c, τ_σ, population, offspring, F_µ, F_λ, C, s, s_σ, σ, E, W, x)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c = CMAES()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function step!(c::CMAES; obj=objective, visualize=false, anim=Nothing)\n",
    "    # L1\n",
    "    sqrt_c = cholesky((c.C + c.C') / 2.0).U\n",
    "    for i in 1:c.λ\n",
    "        c.E[:,i] = randn(c.N)\n",
    "        c.W[:,i] = c.σ * (sqrt_c * c.E[:,i])\n",
    "        c.offspring[i] = c.x + c.W[:,i]\n",
    "        c.F_λ[i] = obj(c.offspring[i])\n",
    "    end    \n",
    "    # Select new parent population\n",
    "    idx = sortperm(c.F_λ)[1:c.μ]\n",
    "    for i in 1:c.μ\n",
    "        c.population[i] = c.offspring[idx[i]]\n",
    "        c.F_μ[i] = c.F_λ[idx[i]]\n",
    "    end    \n",
    "    # L2\n",
    "    w = vec(mean(c.W[:,idx], dims=2))\n",
    "    c.x += w    \n",
    "    # L3\n",
    "    c.s = (1.0 - 1.0/c.τ)*c.s + (sqrt(c.μ/c.τ * (2.0 - 1.0/c.τ))/c.σ)*w   \n",
    "    # L4\n",
    "    c.C = (1.0 - 1.0/c.τ_c).*c.C + (c.s./c.τ_c)*c.s'    \n",
    "    # L5\n",
    "    ɛ = vec(mean(c.E[:,idx], dims=2))\n",
    "    c.s_σ = (1.0 - 1.0/c.τ_σ)*c.s_σ + sqrt(c.μ/c.τ_σ*(2.0 - 1.0/c.τ_σ))*ɛ    \n",
    "    # L6\n",
    "    c.σ = c.σ*exp(((c.s_σ'*c.s_σ)[1] - c.N)/(2*c.N*sqrt(c.N)))\n",
    "    if visualize\n",
    "        plot(xs, ys, fz, st=:contour)\n",
    "        scatter!([c.offspring[i][1] for i in 1:λ], [c.offspring[i][2] for i in 1:λ], \n",
    "            xlims=(-5, 5), ylims=(-5, 5), legend=:none)\n",
    "        scatter!([c.x[1]], [c.x[2]], color=:black, marker=:rect,\n",
    "            xlims=(-5, 5), ylims=(-5, 5), legend=:none)\n",
    "        frame(anim)\n",
    "    end\n",
    "    c\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function plot_obj()\n",
    "    xs = -5.0:0.1:5.0\n",
    "    ys = -5.0:0.1:5.0\n",
    "    fz(x, y) = objective([x, y])\n",
    "\n",
    "    c = CMAES()\n",
    "    println(\"x initial: \", c.x)\n",
    "    anim = Animation()\n",
    "    for i in 1:100\n",
    "        v = mod(i, 1) == 0\n",
    "        step!(c, visualize=v, anim=anim)\n",
    "    end\n",
    "    println(\"x final: \", c.x)\n",
    "    gif(anim)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective = sphere # sphere, himmelblau, styblinski_tang, rastrigin\n",
    "println(solution) # optimal for sphere and rastrigin\n",
    "plot_obj()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>Exercise</b>\n",
    "    <br/>\n",
    "    Modify the parent population size and child population of the problem. Can you get CMA-ES to reliably converge on the Rastrigin function? What about for higer dimensions of $n$? Report your $\\mu$ and $\\lambda$ values in the class chat.\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.4.1",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
