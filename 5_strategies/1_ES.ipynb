{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../imgs/logo.png\" width=\"20%\" align=\"right\" style=\"margin:0px 20px\">\n",
    "\n",
    "\n",
    "# Evolutionary Computation\n",
    "\n",
    "## 5.1 Evolutionary Strategies\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-sa/4.0/80x15.png\" /></a>&nbsp;| Dennis G. Wilson | <a href=\"https://d9w.github.io/evolution/\">https://d9w.github.io/evolution/</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <a id=\"es\"></a>Evolutionary Strategies\n",
    "\n",
    "Evolutionary strategies are a class of evolutionary algorithms first developed in the 1960s. These algorithms are notable for their statistical recombination of the population to inform the next generation and are most often used for continuous optimization problem. Evolutionary Strategies such as CMA-ES, which we'll discuss next, have a long history of application in a variety of domains and recently have been applied to the optimization of neural network weights.\n",
    "\n",
    "+ I. Rechenberg and M. Eigen. Evolutionsstrategie: Optimierung Technischer Systeme nach Prinzipien der Biologischen Evolution. Frommann-Holzboog Stuttgart, 1973.\n",
    "+ Wierstra, Daan, et al. \"Natural evolution strategies.\" The Journal of Machine Learning Research 15.1 (2014): 949-980. [PDF](http://www.jmlr.org/papers/volume15/wierstra14a/wierstra14a.pdf)\n",
    "+ Salimans, Tim, et al. \"Evolution strategies as a scalable alternative to reinforcement learning.\" arXiv preprint arXiv:1703.03864 (2017)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "using Statistics\n",
    "using LinearAlgebra\n",
    "using Plots\n",
    "gr(reuse=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's start by optimizing over a sphere function with an offset center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "s = [3.5, -0.2]\n",
    "sphere(x::Array{Float64}) = -sum((x .- s).^2)\n",
    "objective = sphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "xs = -5:0.1:5\n",
    "ys = -5:0.1:5\n",
    "fz(x, y) = objective([x, y])\n",
    "plot(plot(xs, ys, fz, st=:surface), plot(xs, ys, fz, st=:contour), size = (800, 300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We'll start with a very simple example of an evolutionary strategy. In this $(1,\\lambda)$ Evolutionary Strategy, we will first create random individuals around an expert with a standard deviation of $\\sigma$. This is our population. Let's define some parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>Notation: $(\\mu/\\rho,\\lambda)$ or $(\\mu/\\rho+\\lambda)$</b>\n",
    "    <br/>\n",
    "    In this notation, $\\mu$ is the number of parents, $\\rho$ is the number of parents involved in creating the offspring, and $\\lambda$ is the number of offspring. $(\\mu/\\rho+\\lambda)$ means the parents are included for creating the next population, $(\\mu/\\rho,\\lambda)$ means the parents are not included. We've already seen the $(1+1)$ and $(1+\\lambda)$ algorithms, where the 1 parent is included in the next population based on its fitness. CMA-ES is a $(\\mu/\\mu_I,\\lambda)$ algorithm where a transformation of $\\mu$, $\\mu_I$, will inform the next population.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "npop = 50     # population size\n",
    "sigma = 0.1   # noise standard deviation\n",
    "alpha = 0.001 # step size\n",
    "x = randn(2)  # initial expert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Since we'll be staying in the continuous optimization domain today, we won't define an `Individual` class but will use matrices for our population. Let's now create the population around `x` with standard deviation $\\sigma$. We will use a Normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "N = randn(npop, 2)\n",
    "P = repeat(x, 1, npop)' .+ sigma .* N;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now we have our population and can evaluate them. We will also normalize the evaluation $f(x)$ using $\\frac{f(x) - \\mu(f(x))}{\\sigma(f(x))}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "R = zeros(npop)\n",
    "for i in eachindex(R)\n",
    "    R[i] = objective(P[i, :])\n",
    "end\n",
    "A = (R .- mean(R)) ./ std(R);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's see how our population looks on our objective function. The black rectangle is our current expert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "xs = floor(minimum(P[:,1]), digits=1):0.1:ceil(maximum(P[:,1]), digits=1)\n",
    "ys = floor(minimum(P[:,2]), digits=1):0.1:ceil(maximum(P[:,2]), digits=1)\n",
    "fz(x, y) = objective([x, y])\n",
    "plot(xs, ys, fz, st=:contour)\n",
    "scatter!(P[:, 1], P[:, 2], zcolor=R, legend=:none)\n",
    "scatter!([x[1]], [x[2]], color=:black, marker=:rect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As we can see, some individuals in our distribution are better than our current expert. We'll shift towards those individuals by multiplying each individual in $N$ by their normalized fitness $A$, then summing over all individuals. This gives us the direction of the gradient based on our population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "(dot(N[:, 1], A) / npop, dot(N[:, 2], A) / npop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now that we know which direction to move in, we apply this step with a step size of $\\alpha$. In other words, we move $\\alpha$ along our approximation of the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "println(\"Solution :\", s)\n",
    "println(\"x :\", x)\n",
    "x = x .+ alpha/(npop * sigma) .* [dot(N[:, i], A) for i in 1:size(N, 2)]\n",
    "println(\"x`: \", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As we can see, we've moved closer to the objective. Let's plot this over multiple steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "xs = -5.0:0.1:5.0\n",
    "ys = -5.0:0.1:5.0\n",
    "fz(x, y) = objective([x, y]);\n",
    "\n",
    "function step(x::Array{Float64}; npop=50, sigma=0.1, alpha=0.01, visualize=false, anim=Nothing)\n",
    "    N = randn(npop, 2)\n",
    "    P = repeat(x, 1, npop)' .+ sigma .* N\n",
    "    R = zeros(npop)\n",
    "    for i in eachindex(R)\n",
    "        R[i] = objective(P[i, :])\n",
    "    end\n",
    "    A = (R .- mean(R)) ./ std(R)\n",
    "    \n",
    "    if visualize\n",
    "        plot(xs, ys, fz, st=:contour)\n",
    "        scatter!(P[:, 1], P[:, 2], xlims=(-5, 5), ylims=(-5, 5), zcolor=R)\n",
    "        scatter!([x[1]], [x[2]], legend=:none, color=:black, marker=:rect)\n",
    "        frame(anim)\n",
    "    end\n",
    "    \n",
    "    x .+ alpha/(npop * sigma) .* [dot(N[:, i], A) for i in 1:size(N, 2)]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "function plot_obj()\n",
    "    x = randn(2)\n",
    "    println(\"x initial: \", x)\n",
    "    anim = Animation()\n",
    "    for i in 1:500\n",
    "        v = mod(i, 10) == 0\n",
    "        x = step(x, npop=50, sigma=0.1, alpha=0.001, visualize=v, anim=anim)\n",
    "    end\n",
    "    println(\"x final: \", x)\n",
    "    gif(anim)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's also define some other objective functions just for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "himmelblau(x::Array{Float64}) = -((x[1]^2 + x[2] - 11)^2 + (x[1] + x[2]^2 - 7)^2)\n",
    "styblinski_tang(x::Array{Float64}) = -(sum(x.^4 .- 16 .* x.^2 .+ 5 .* x) / 2.0)\n",
    "rastrigin(x::Array{Float64}) = -(10.0 * length(x) .+ sum((x .- s).^2 .- 10 .* cos.(2*pi.*(x .- s))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "objective = rastrigin # sphere, himmelblau, styblinski_tang, rastrigin\n",
    "plot_obj()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <a id=\"edas\"></a>Estimation of Distribution Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "EDAs are algorithms that build a probabilistic model of candidate solutions. These are not the subject of today's lecture, but are similar and worth looking into.\n",
    "    \n",
    "+ Mühlenbein, Heinz, and Gerhard Paass. \"From recombination of genes to the estimation of distributions I. Binary parameters.\" International conference on parallel problem solving from nature. Springer, Berlin, Heidelberg, 1996.\n",
    "+ Larrañaga, Pedro, and Jose A. Lozano, eds. Estimation of distribution algorithms: A new tool for evolutionary computation. Vol. 2. Springer Science & Business Media, 2001.\n",
    "+ Pelikan, Martin, David E. Goldberg, and Fernando G. Lobo. \"A survey of optimization by building and using probabilistic models.\" Computational optimization and applications 21.1 (2002): 5-20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../imgs/EDAs.png\" width=\"60%\">\n",
    "Hauschild, Mark, and Martin Pelikan. \"An introduction and survey of estimation of distribution algorithms.\" Swarm and evolutionary computation 1.3 (2011): 111-128."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.4.1",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
